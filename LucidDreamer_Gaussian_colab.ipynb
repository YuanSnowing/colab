{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuanSnowing/colab/blob/main/LucidDreamer_Gaussian_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR",
        "outputId": "3b06db38-f521-4f98-d58a-bcba733725c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LucidDreamer-Gaussian'...\n",
            "remote: Enumerating objects: 442, done.\u001b[K\n",
            "remote: Counting objects: 100% (145/145), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 442 (delta 124), reused 103 (delta 103), pack-reused 297 (from 1)\u001b[K\n",
            "Receiving objects: 100% (442/442), 62.50 MiB | 24.18 MiB/s, done.\n",
            "Resolving deltas: 100% (187/187), done.\n",
            "Collecting timm==0.6.7\n",
            "  Downloading timm-0.6.7-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.6.7) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.6.7) (0.20.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.7) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.7) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.7) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.7) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.7) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.7) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.4->timm==0.6.7) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.6.7) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.6.7) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm==0.6.7) (3.0.2)\n",
            "Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.11\n",
            "    Uninstalling timm-1.0.11:\n",
            "      Successfully uninstalled timm-1.0.11\n",
            "Successfully installed timm-0.6.7\n",
            "Collecting peft\n",
            "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.30.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Collecting open3d\n",
            "  Downloading open3d-0.18.0-cp310-cp310-manylinux_2_27_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting torch==2.0.1\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchvision==0.15.2\n",
            "  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.3.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: imageio[ffmpeg] in /usr/local/lib/python3.10/dist-packages (2.35.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.30.5)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.24.7)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.9.11)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (0.5.1)\n",
            "Collecting dash>=2.6.0 (from open3d)\n",
            "  Downloading dash-2.18.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: werkzeug>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.0.4)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (5.10.4)\n",
            "Collecting configargparse (from open3d)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from open3d)\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting addict (from open3d)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.7.1)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.5.2)\n",
            "Collecting pyquaternion (from open3d)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.3-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.4.2 (from gradio)\n",
            "  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting huggingface-hub>=0.17.0 (from peft)\n",
            "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.7.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.2->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.2.5)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n",
            "Collecting dash-html-components==2.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dash-table==5.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting retrying (from dash>=2.6.0->open3d)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (2.8.2)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (3.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.20.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (2.2.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (2.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.20.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.6)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n",
            "Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "# nn.functional.interpolate(x, (int(height), int(width)), mode='bilinear', align_corners=True)\n",
        "# annotator/zoe/zoedepth/models/base_models/midas.py\n",
        "\n",
        "%cd /content\n",
        "!git clone --recursive -b dev https://github.com/camenduru/LucidDreamer-Gaussian\n",
        "!pip install timm==0.6.7\n",
        "!pip install peft diffusers scipy numpy imageio[ffmpeg] opencv-python Pillow open3d torch==2.0.1 torchvision==0.15.2 gradio omegaconf\n",
        "\n",
        "!pip install plyfile==0.8.1\n",
        "!pip install -q diffusers accelerate gradio open3d plyfile timm==0.6.12\n",
        "!pip install -q https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl\n",
        "\n",
        "!apt install libglm-dev\n",
        "!pip install /content/LucidDreamer-Gaussian/submodules/depth-diff-gaussian-rasterization-min\n",
        "!pip install /content/LucidDreamer-Gaussian/submodules/simple-knn\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/ZoeDepth/resolve/main/ZoeD_M12_N.pt -d /root/.cache/torch/hub/checkpoints -o ZoeD_M12_N.pt\n",
        "\n",
        "!sed -i 's/\\(new_height, new_width\\)/int(new_height), int(new_width)/g' /content/LucidDreamer-Gaussian/ZoeDepth/zoedepth/models/base_models/midas.py\n",
        "\n",
        "import torch\n",
        "\n",
        "torch.hub.load('LucidDreamer-Gaussian/ZoeDepth', 'ZoeD_N', source='local', pretrained=True, strict=False).to('cuda')\n",
        "!sed -i 's/\\(new_height, new_width\\)/int(new_height), int(new_width)/g' /root/.cache/torch/hub/intel-isl_MiDaS_master/midas/backbones/beit.py\n",
        "\n",
        "%cd /content/LucidDreamer-Gaussian\n",
        "import os\n",
        "import glob\n",
        "import platform\n",
        "import pathlib\n",
        "import shlex\n",
        "import subprocess\n",
        "import gradio as gr\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "\n",
        "root = '/content/LucidDreamer-Gaussian'\n",
        "example_root = os.path.join(root, 'examples')\n",
        "ckpt_root = os.path.join(root, 'stablediffusion')\n",
        "use_symlinks = False\n",
        "\n",
        "d = example_root\n",
        "if len(glob.glob(os.path.join(d, '*.ply'))) < 8:\n",
        "    snapshot_download(repo_id=\"ironjr/LucidDreamerDemo\", repo_type=\"model\", local_dir=d, local_dir_use_symlinks=use_symlinks)\n",
        "d = os.path.join(ckpt_root, 'Blazing Drive V11m')\n",
        "if not os.path.exists(d):\n",
        "    snapshot_download(repo_id=\"ironjr/BlazingDriveV11m\", repo_type=\"model\", local_dir=d, local_dir_use_symlinks=use_symlinks)\n",
        "d = os.path.join(ckpt_root, 'RealCartoon-Pixar V5')\n",
        "if not os.path.exists(d):\n",
        "    snapshot_download(repo_id=\"ironjr/RealCartoon-PixarV5\", repo_type=\"model\", local_dir=d, local_dir_use_symlinks=use_symlinks)\n",
        "d = os.path.join(ckpt_root, 'Realistic Vision V5.1')\n",
        "if not os.path.exists(d):\n",
        "    snapshot_download(repo_id=\"ironjr/RealisticVisionV5-1\", repo_type=\"model\", local_dir=d, local_dir_use_symlinks=use_symlinks)\n",
        "d = os.path.join(ckpt_root, 'SD1-5')\n",
        "if not os.path.exists(d):\n",
        "    snapshot_download(repo_id=\"runwayml/stable-diffusion-inpainting\", repo_type=\"model\", local_dir=d, local_dir_use_symlinks=use_symlinks)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# Copyright (C) 2023, Computer Vision Lab, Seoul National University, https://cv.snu.ac.kr\n",
        "# For permission requests, please contact robot0321@snu.ac.kr, esw0116@snu.ac.kr, namhj28@gmail.com, jarin.lee@gmail.com.\n",
        "# All rights reserved.\n",
        "###\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class GSParams:\n",
        "    def __init__(self):\n",
        "        self.sh_degree = 3\n",
        "        self.images = \"images\"\n",
        "        self.resolution = -1\n",
        "        self.white_background = False\n",
        "        self.data_device = \"cuda\"\n",
        "        self.eval = False\n",
        "        self.use_depth = False\n",
        "\n",
        "        self.iterations = 2990#3_000\n",
        "        self.position_lr_init = 0.00016\n",
        "        self.position_lr_final = 0.0000016\n",
        "        self.position_lr_delay_mult = 0.01\n",
        "        self.position_lr_max_steps = 2990#3_000\n",
        "        self.feature_lr = 0.0025\n",
        "        self.opacity_lr = 0.05\n",
        "        self.scaling_lr = 0.005\n",
        "        self.rotation_lr = 0.001\n",
        "        self.percent_dense = 0.01\n",
        "        self.lambda_dssim = 0.2\n",
        "        self.densification_interval = 100\n",
        "        self.opacity_reset_interval = 3000\n",
        "        self.densify_from_iter = 500\n",
        "        self.densify_until_iter = 15_000\n",
        "        self.densify_grad_threshold = 0.0002\n",
        "\n",
        "        self.convert_SHs_python = False\n",
        "        self.compute_cov3D_python = False\n",
        "        self.debug = False\n",
        "\n",
        "\n",
        "class CameraParams:\n",
        "    def __init__(self, H: int = 512, W: int = 512):\n",
        "        self.H = H\n",
        "        self.W = W\n",
        "        self.focal = (5.8269e+02, 5.8269e+02)\n",
        "        self.fov = (2*np.arctan(self.W / (2*self.focal[0])), 2*np.arctan(self.H / (2*self.focal[1])))\n",
        "        self.K = np.array([\n",
        "            [self.focal[0], 0., self.W/2],\n",
        "            [0., self.focal[1], self.H/2],\n",
        "            [0.,            0.,       1.],\n",
        "        ]).astype(np.float32)"
      ],
      "metadata": {
        "id": "slP1B-I-leoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "from random import randint\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "import imageio\n",
        "import numpy as np\n",
        "import open3d as o3d\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from scipy.interpolate import griddata as interp_grid\n",
        "from scipy.ndimage import minimum_filter, maximum_filter\n",
        "\n",
        "import torch\n",
        "import gradio as gr\n",
        "from diffusers import StableDiffusionInpaintPipeline, StableDiffusionPipeline\n",
        "from gaussian_renderer import render\n",
        "from scene import Scene, GaussianModel\n",
        "from scene.dataset_readers import loadCameraPreset\n",
        "from utils.loss import l1_loss, ssim\n",
        "from utils.camera import load_json\n",
        "from utils.trajectory import get_camerapaths, get_pcdGenPoses\n",
        "\n",
        "\n",
        "class LucidDreamer:\n",
        "  def __init__(self, savepath='/content/LucidDreamer-Gaussian/'):\n",
        "      self.opt = GSParams()\n",
        "      self.cam = CameraParams()\n",
        "      self.savepath = savepath\n",
        "      if not os.path.exists(self.savepath):\n",
        "          os.makedirs(self.savepath, exist_ok=True)\n",
        "\n",
        "      self.gaussians = GaussianModel(self.opt.sh_degree)\n",
        "\n",
        "      bg_color = [1, 1, 1] if self.opt.white_background else [0, 0, 0]\n",
        "      self.background = torch.tensor(bg_color, dtype=torch.float32, device='cuda')\n",
        "\n",
        "      self.rgb = StableDiffusionInpaintPipeline.from_pretrained(\n",
        "          '/content/LucidDreamer-Gaussian/stablediffusion/SD1-5', revision='fp16', torch_dtype=torch.float16).to('cuda')\n",
        "      self.rgb.enable_xformers_memory_efficient_attention()\n",
        "      self.d = torch.hub.load('./ZoeDepth', 'ZoeD_N', source='local', pretrained=True).to('cuda')\n",
        "\n",
        "  def run(self, rgb_cond, txt_cond, neg_txt_cond, pcdgenpath, seed, diff_steps, render_camerapath):\n",
        "      gaussians, default_gallery = self.create(rgb_cond, txt_cond, neg_txt_cond, pcdgenpath, seed, diff_steps)\n",
        "      gallery = self.render_video(render_camerapath)\n",
        "      return (gaussians, default_gallery, gallery)\n",
        "\n",
        "  def create(self, rgb_cond, txt_cond, neg_txt_cond, pcdgenpath, seed, diff_steps):\n",
        "      self.traindata = self.generate_pcd(rgb_cond, txt_cond, neg_txt_cond, pcdgenpath, seed, diff_steps)\n",
        "      self.scene = Scene(self.traindata, self.gaussians, self.opt)\n",
        "      self.training()\n",
        "      outfile = self.save_ply()\n",
        "      default_gallery = self.render_video('LLFF')\n",
        "      return outfile, default_gallery\n",
        "\n",
        "  def save_ply(self):\n",
        "      self.scene.gaussians.save_ply(os.path.join(self.savepath, 'gaussiansplatting.ply'))\n",
        "      return os.path.join(self.savepath, 'gaussiansplatting.ply')\n",
        "\n",
        "  def render_video(self, preset, gaussians=None, traindata_path=None):\n",
        "      videofname = os.path.join(self.savepath, f'video_{preset}_60fps.mp4')\n",
        "\n",
        "      if not hasattr(self, 'gaussians'):\n",
        "          self.gaussians = gaussians\n",
        "\n",
        "      if not hasattr(self, 'scene'):\n",
        "          print(traindata_path)\n",
        "\n",
        "          with open(traindata_path, 'r') as f:\n",
        "              traindata = json.load(traindata_path)\n",
        "\n",
        "          print(traindata)\n",
        "\n",
        "          view_total = loadCameraPreset(traindata=traindata, presetdata=get_camerapaths())\n",
        "          views = view_total[preset]\n",
        "          print(view_total.keys())\n",
        "      else:\n",
        "          views = self.scene.getPresetCameras(preset)\n",
        "\n",
        "      framelist = []\n",
        "      depthlist = []\n",
        "      dmin, dmax = 1e3, 0\n",
        "      for view in views:\n",
        "          results = render(view, self.gaussians, self.opt, self.background)\n",
        "          rendering = results['render']\n",
        "\n",
        "          dmask = (results['depth']>0)\n",
        "          depth = (results['depth']*dmask).detach().cpu().numpy()\n",
        "          depthlist.append(depth)\n",
        "          if results['depth'][dmask].min().item() < dmin:\n",
        "              dmin = results['depth'][dmask].min().item()\n",
        "          if results['depth'][dmask].max().item() > dmax:\n",
        "              dmax = results['depth'][dmask].max().item()\n",
        "\n",
        "          framelist.append(np.round(rendering.permute(1,2,0).detach().cpu().numpy().clip(0,1)*255.).astype(np.uint8))\n",
        "\n",
        "      imageio.mimwrite(videofname, framelist, fps=60, quality=8)\n",
        "      return videofname\n",
        "\n",
        "  def training(self):\n",
        "      if not self.scene:\n",
        "          raise('Build 3D Scene First!')\n",
        "\n",
        "      for iteration in tqdm(range(1, self.opt.iterations + 1)):\n",
        "          self.gaussians.update_learning_rate(iteration)\n",
        "\n",
        "          # Every 1000 its we increase the levels of SH up to a maximum degree\n",
        "          if iteration % 1000 == 0:\n",
        "              self.gaussians.oneupSHdegree()\n",
        "\n",
        "          # Pick a random Camera\n",
        "          viewpoint_stack = self.scene.getTrainCameras().copy()\n",
        "          viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack)-1))\n",
        "\n",
        "          # Render\n",
        "          render_pkg = render(viewpoint_cam, self.gaussians, self.opt, self.background)\n",
        "          image, viewspace_point_tensor, visibility_filter, radii = (\n",
        "              render_pkg['render'], render_pkg['viewspace_points'], render_pkg['visibility_filter'], render_pkg['radii'])\n",
        "\n",
        "          # Loss\n",
        "          gt_image = viewpoint_cam.original_image.cuda()\n",
        "          Ll1 = l1_loss(image, gt_image)\n",
        "          loss = (1.0 - self.opt.lambda_dssim) * Ll1 + self.opt.lambda_dssim * (1.0 - ssim(image, gt_image))\n",
        "          loss.backward()\n",
        "\n",
        "          with torch.no_grad():\n",
        "              # Densification\n",
        "              if iteration < self.opt.densify_until_iter:\n",
        "                  # Keep track of max radii in image-space for pruning\n",
        "                  self.gaussians.max_radii2D[visibility_filter] = torch.max(\n",
        "                      self.gaussians.max_radii2D[visibility_filter], radii[visibility_filter])\n",
        "                  self.gaussians.add_densification_stats(viewspace_point_tensor, visibility_filter)\n",
        "\n",
        "                  if iteration > self.opt.densify_from_iter and iteration % self.opt.densification_interval == 0:\n",
        "                      size_threshold = 20 if iteration > self.opt.opacity_reset_interval else None\n",
        "                      self.gaussians.densify_and_prune(\n",
        "                          self.opt.densify_grad_threshold, 0.005, self.scene.cameras_extent, size_threshold)\n",
        "\n",
        "                  if (iteration % self.opt.opacity_reset_interval == 0\n",
        "                      or (self.opt.white_background and iteration == self.opt.densify_from_iter)\n",
        "                  ):\n",
        "                      self.gaussians.reset_opacity()\n",
        "\n",
        "              # Optimizer step\n",
        "              if iteration < self.opt.iterations:\n",
        "                  self.gaussians.optimizer.step()\n",
        "                  self.gaussians.optimizer.zero_grad(set_to_none = True)\n",
        "\n",
        "  def generate_pcd(self, rgb_cond, prompt, negative_prompt, pcdgenpath, seed, diff_steps, progress=gr.Progress()):\n",
        "      ## processing inputs\n",
        "      generator=torch.Generator(device='cuda').manual_seed(seed)\n",
        "\n",
        "      w_in, h_in = rgb_cond.size\n",
        "      if w_in/h_in > 1.1 or h_in/w_in > 1.1: # if height and width are similar, do center crop\n",
        "          in_res = max(w_in, h_in)\n",
        "          image_in, mask_in = np.zeros((in_res, in_res, 3), dtype=np.uint8), 255*np.ones((in_res, in_res, 3), dtype=np.uint8)\n",
        "          image_in[int(in_res/2-h_in/2):int(in_res/2+h_in/2), int(in_res/2-w_in/2):int(in_res/2+w_in/2)] = np.array(rgb_cond)\n",
        "          mask_in[int(in_res/2-h_in/2):int(in_res/2+h_in/2), int(in_res/2-w_in/2):int(in_res/2+w_in/2)] = 0\n",
        "          image_curr = self.rgb(prompt=prompt, negative_prompt=negative_prompt, generator=generator,\n",
        "                            image=Image.fromarray(image_in).resize((self.cam.W, self.cam.H)),\n",
        "                            mask_image=Image.fromarray(mask_in).resize((self.cam.W, self.cam.H))).images[0]\n",
        "\n",
        "      else: # if there is a large gap between height and width, do inpainting\n",
        "          if w_in > h_in:\n",
        "              image_curr = rgb_cond.crop((int(w_in/2-h_in/2), 0, int(w_in/2+h_in/2), h_in)).resize((self.cam.W, self.cam.H))\n",
        "          else: # w <= h\n",
        "              image_curr = rgb_cond.crop((0, int(h_in/2-w_in/2), w_in, int(h_in/2+w_in/2))).resize((self.cam.W, self.cam.H))\n",
        "\n",
        "      render_poses = get_pcdGenPoses(pcdgenpath)\n",
        "      depth_curr = self.d.infer_pil(image_curr)\n",
        "      center_depth = np.mean(depth_curr[h_in//2-10:h_in//2+10, w_in//2-10:w_in//2+10])\n",
        "\n",
        "      ###########################################################################################################################\n",
        "      # Iterative scene generation\n",
        "      H, W, K = self.cam.H, self.cam.W, self.cam.K\n",
        "\n",
        "      x, y = np.meshgrid(np.arange(W, dtype=np.float32), np.arange(H, dtype=np.float32), indexing='xy') # pixels\n",
        "      edgeN = 2\n",
        "      edgemask = np.ones((H-2*edgeN, W-2*edgeN))\n",
        "      edgemask = np.pad(edgemask, ((edgeN,edgeN),(edgeN,edgeN)))\n",
        "\n",
        "      ### initialize\n",
        "      R0, T0 = render_poses[0,:3,:3], render_poses[0,:3,3:4]\n",
        "      pts_coord_cam = np.matmul(np.linalg.inv(K), np.stack((x*depth_curr, y*depth_curr, 1*depth_curr), axis=0).reshape(3,-1))\n",
        "      new_pts_coord_world2 = (np.linalg.inv(R0).dot(pts_coord_cam) - np.linalg.inv(R0).dot(T0)).astype(np.float32)\n",
        "      new_pts_colors2 = (np.array(image_curr).reshape(-1,3).astype(np.float32)/255.)\n",
        "\n",
        "      pts_coord_world, pts_colors = new_pts_coord_world2.copy(), new_pts_colors2.copy()\n",
        "\n",
        "      progress(0, desc='Dreaming...')\n",
        "      time.sleep(0.5)\n",
        "\n",
        "      for j, tqdm_i in enumerate(progress.tqdm(range(1, len(render_poses)), desc='Dreaming')):\n",
        "          i = j + 1\n",
        "          if i == len(render_poses) - 1:\n",
        "              break\n",
        "          R, T = render_poses[i,:3,:3], render_poses[i,:3,3:4]\n",
        "\n",
        "          ### Transform world to pixel\n",
        "          pts_coord_cam2 = R.dot(pts_coord_world) + T\n",
        "          pixel_coord_cam2 = np.matmul(K, pts_coord_cam2)\n",
        "\n",
        "          valid_idx = np.where(np.logical_and.reduce((pixel_coord_cam2[2]>0,\n",
        "                                                      pixel_coord_cam2[0]/pixel_coord_cam2[2]>=0,\n",
        "                                                      pixel_coord_cam2[0]/pixel_coord_cam2[2]<=W-1,\n",
        "                                                      pixel_coord_cam2[1]/pixel_coord_cam2[2]>=0,\n",
        "                                                      pixel_coord_cam2[1]/pixel_coord_cam2[2]<=H-1)))[0]\n",
        "          pixel_coord_cam2 = pixel_coord_cam2[:2, valid_idx]/pixel_coord_cam2[-1:, valid_idx]\n",
        "          round_coord_cam2 = np.round(pixel_coord_cam2).astype(np.int32)\n",
        "\n",
        "          x, y = np.meshgrid(np.arange(W, dtype=np.float32), np.arange(H, dtype=np.float32), indexing='xy')\n",
        "          grid = np.stack((x,y), axis=-1).reshape(-1,2)\n",
        "          image2 = interp_grid(pixel_coord_cam2.transpose(1,0), pts_colors[valid_idx], grid, method='linear', fill_value=0).reshape(H,W,3)\n",
        "          image2 = edgemask[...,None]*image2 + (1-edgemask[...,None])*np.pad(image2[1:-1,1:-1], ((1,1),(1,1),(0,0)), mode='edge')\n",
        "\n",
        "          round_mask2 = np.zeros((H,W), dtype=np.float32)\n",
        "          round_mask2[round_coord_cam2[1], round_coord_cam2[0]] = 1\n",
        "\n",
        "          round_mask2 = maximum_filter(round_mask2, size=(9,9), axes=(0,1))\n",
        "          image2 = round_mask2[...,None]*image2 + (1-round_mask2[...,None])*(-1)\n",
        "\n",
        "          mask2 = minimum_filter((image2.sum(-1)!=-3)*1, size=(11,11), axes=(0,1))\n",
        "          image2 = mask2[...,None]*image2 + (1-mask2[...,None])*0\n",
        "\n",
        "          mask_hf = np.abs(mask2[:H-1, :W-1] - mask2[1:, :W-1]) + np.abs(mask2[:H-1, :W-1] - mask2[:H-1, 1:])\n",
        "          mask_hf = np.pad(mask_hf, ((0,1), (0,1)), 'edge')\n",
        "          mask_hf = np.where(mask_hf < 0.3, 0, 1)\n",
        "          border_valid_idx = np.where(mask_hf[round_coord_cam2[1], round_coord_cam2[0]] == 1)[0]\n",
        "\n",
        "          image_curr = self.rgb(prompt=prompt, negative_prompt=negative_prompt, generator=generator, num_inference_steps= diff_steps,\n",
        "                                  image=Image.fromarray(np.round(image2*255.).astype(np.uint8)),\n",
        "                                  mask_image=Image.fromarray(np.round((1-mask2[:,:])*255.).astype(np.uint8))).images[0]\n",
        "          depth_curr = self.d.infer_pil(image_curr)\n",
        "\n",
        "          t_z2 = torch.tensor(depth_curr)\n",
        "          sc = torch.ones(1).float().requires_grad_(True)\n",
        "          optimizer = torch.optim.Adam(params=[sc], lr=0.001)\n",
        "\n",
        "          for idx in range(100):\n",
        "              trans3d = torch.tensor([[sc,0,0,0], [0,sc,0,0], [0,0,sc,0], [0,0,0,1]]).requires_grad_(True)\n",
        "              coord_cam2 = torch.matmul(torch.tensor(np.linalg.inv(K)), torch.stack((torch.tensor(x)*t_z2, torch.tensor(y)*t_z2, 1*t_z2), axis=0)[:,round_coord_cam2[1], round_coord_cam2[0]].reshape(3,-1))\n",
        "              coord_world2 = (torch.tensor(np.linalg.inv(R)).float().matmul(coord_cam2) - torch.tensor(np.linalg.inv(R)).float().matmul(torch.tensor(T).float()))\n",
        "              coord_world2_warp = torch.cat((coord_world2, torch.ones((1,valid_idx.shape[0]))), dim=0)\n",
        "              coord_world2_trans = torch.matmul(trans3d, coord_world2_warp)\n",
        "              coord_world2_trans = coord_world2_trans[:3] / coord_world2_trans[-1]\n",
        "              loss = torch.mean((torch.tensor(pts_coord_world[:,valid_idx]).float() - coord_world2_trans)**2)\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "          with torch.no_grad():\n",
        "              coord_cam2 = torch.matmul(torch.tensor(np.linalg.inv(K)), torch.stack((torch.tensor(x)*t_z2, torch.tensor(y)*t_z2, 1*t_z2), axis=0)[:,round_coord_cam2[1, border_valid_idx], round_coord_cam2[0, border_valid_idx]].reshape(3,-1))\n",
        "              coord_world2 = (torch.tensor(np.linalg.inv(R)).float().matmul(coord_cam2) - torch.tensor(np.linalg.inv(R)).float().matmul(torch.tensor(T).float()))\n",
        "              coord_world2_warp = torch.cat((coord_world2, torch.ones((1, border_valid_idx.shape[0]))), dim=0)\n",
        "              coord_world2_trans = torch.matmul(trans3d, coord_world2_warp)\n",
        "              coord_world2_trans = coord_world2_trans[:3] / coord_world2_trans[-1]\n",
        "\n",
        "          trans3d = trans3d.detach().numpy()\n",
        "\n",
        "          pts_coord_cam2 = np.matmul(np.linalg.inv(K), np.stack((x*depth_curr, y*depth_curr, 1*depth_curr), axis=0).reshape(3,-1))[:,np.where(1-mask2.reshape(-1))[0]]\n",
        "          camera_origin_coord_world2 = - np.linalg.inv(R).dot(T).astype(np.float32)\n",
        "          new_pts_coord_world2 = (np.linalg.inv(R).dot(pts_coord_cam2) - np.linalg.inv(R).dot(T)).astype(np.float32)\n",
        "          new_pts_coord_world2_warp = np.concatenate((new_pts_coord_world2, np.ones((1, new_pts_coord_world2.shape[1]))), axis=0)\n",
        "          new_pts_coord_world2 = np.matmul(trans3d, new_pts_coord_world2_warp)\n",
        "          new_pts_coord_world2 = new_pts_coord_world2[:3] / new_pts_coord_world2[-1]\n",
        "          new_pts_colors2 = (np.array(image_curr).reshape(-1,3).astype(np.float32)/255.)[np.where(1-mask2.reshape(-1))[0]]\n",
        "\n",
        "          vector_camorigin_to_campixels = coord_world2_trans.detach().numpy() - camera_origin_coord_world2\n",
        "          vector_camorigin_to_pcdpixels = pts_coord_world[:,valid_idx[border_valid_idx]] - camera_origin_coord_world2\n",
        "\n",
        "          compensate_depth_coeff = np.sum(vector_camorigin_to_pcdpixels * vector_camorigin_to_campixels, axis=0) / np.sum(vector_camorigin_to_campixels * vector_camorigin_to_campixels, axis=0) # N_correspond\n",
        "          compensate_pts_coord_world2_correspond = camera_origin_coord_world2 + vector_camorigin_to_campixels * compensate_depth_coeff.reshape(1,-1)\n",
        "\n",
        "          compensate_coord_cam2_correspond = R.dot(compensate_pts_coord_world2_correspond) + T\n",
        "          homography_coord_cam2_correspond = R.dot(coord_world2_trans.detach().numpy()) + T\n",
        "\n",
        "          compensate_depth_correspond = compensate_coord_cam2_correspond[-1] - homography_coord_cam2_correspond[-1]\n",
        "          compensate_depth_zero = np.zeros(4)\n",
        "          compensate_depth = np.concatenate((compensate_depth_correspond, compensate_depth_zero), axis=0)\n",
        "\n",
        "          pixel_cam2_correspond = pixel_coord_cam2[:, border_valid_idx]\n",
        "          pixel_cam2_zero = np.array([[0,0,W-1,W-1],[0,H-1,0,H-1]])\n",
        "          pixel_cam2 = np.concatenate((pixel_cam2_correspond, pixel_cam2_zero), axis=1).transpose(1,0)\n",
        "\n",
        "          # Calculate for masked pixels\n",
        "          masked_pixels_xy = np.stack(np.where(1-mask2), axis=1)[:, [1,0]]\n",
        "          new_depth_linear, new_depth_nearest = interp_grid(pixel_cam2, compensate_depth, masked_pixels_xy), interp_grid(pixel_cam2, compensate_depth, masked_pixels_xy, method='nearest')\n",
        "          new_depth = np.where(np.isnan(new_depth_linear), new_depth_nearest, new_depth_linear)\n",
        "\n",
        "          pts_coord_cam2 = np.matmul(np.linalg.inv(K), np.stack((x*depth_curr, y*depth_curr, 1*depth_curr), axis=0).reshape(3,-1))[:,np.where(1-mask2.reshape(-1))[0]]\n",
        "          x_nonmask, y_nonmask = x.reshape(-1)[np.where(1-mask2.reshape(-1))[0]], y.reshape(-1)[np.where(1-mask2.reshape(-1))[0]]\n",
        "          compensate_pts_coord_cam2 = np.matmul(np.linalg.inv(K), np.stack((x_nonmask*new_depth, y_nonmask*new_depth, 1*new_depth), axis=0))\n",
        "          new_warp_pts_coord_cam2 = pts_coord_cam2 + compensate_pts_coord_cam2\n",
        "\n",
        "          new_pts_coord_world2 = (np.linalg.inv(R).dot(new_warp_pts_coord_cam2) - np.linalg.inv(R).dot(T)).astype(np.float32)\n",
        "          new_pts_coord_world2_warp = np.concatenate((new_pts_coord_world2, np.ones((1, new_pts_coord_world2.shape[1]))), axis=0)\n",
        "          new_pts_coord_world2 = np.matmul(trans3d, new_pts_coord_world2_warp)\n",
        "          new_pts_coord_world2 = new_pts_coord_world2[:3] / new_pts_coord_world2[-1]\n",
        "          new_pts_colors2 = (np.array(image_curr).reshape(-1,3).astype(np.float32)/255.)[np.where(1-mask2.reshape(-1))[0]]\n",
        "\n",
        "          pts_coord_world = np.concatenate((pts_coord_world, new_pts_coord_world2), axis=-1)\n",
        "          pts_colors = np.concatenate((pts_colors, new_pts_colors2), axis=0)\n",
        "\n",
        "      #################################################################################################\n",
        "\n",
        "      yz_reverse = np.array([[1,0,0], [0,-1,0], [0,0,-1]])\n",
        "      traindata = {\n",
        "          'camera_angle_x': self.cam.fov[0],\n",
        "          'W': W,\n",
        "          'H': H,\n",
        "          'pcd_points': pts_coord_world,\n",
        "          'pcd_colors': pts_colors,\n",
        "          'frames': [],\n",
        "      }\n",
        "\n",
        "      internel_render_poses = get_pcdGenPoses('hemisphere', {'center_depth': center_depth})\n",
        "\n",
        "      progress(0, desc='Aligning...')\n",
        "      time.sleep(0.5)\n",
        "\n",
        "      for i, tqdm_i in enumerate(progress.tqdm(range(len(render_poses)), desc='Aligning')):\n",
        "          if i == len(render_poses) - 1:\n",
        "              break\n",
        "          for j in range(len(internel_render_poses)):\n",
        "              idx = i * len(internel_render_poses) + j\n",
        "              print(f'{idx+1} / {len(render_poses)*len(internel_render_poses)}')\n",
        "\n",
        "              ### Transform world to pixel\n",
        "              Rw2i = render_poses[i,:3,:3]\n",
        "              Tw2i = render_poses[i,:3,3:4]\n",
        "              Ri2j = internel_render_poses[j,:3,:3]\n",
        "              Ti2j = internel_render_poses[j,:3,3:4]\n",
        "\n",
        "              Rw2j = np.matmul(Ri2j, Rw2i)\n",
        "              Tw2j = np.matmul(Ri2j, Tw2i) + Ti2j\n",
        "\n",
        "              # Transfrom cam2 to world + change sign of yz axis\n",
        "              Rj2w = np.matmul(yz_reverse, Rw2j).T\n",
        "              Tj2w = -np.matmul(Rj2w, np.matmul(yz_reverse, Tw2j))\n",
        "              Pc2w = np.concatenate((Rj2w, Tj2w), axis=1)\n",
        "              Pc2w = np.concatenate((Pc2w, np.array([[0,0,0,1]])), axis=0)\n",
        "\n",
        "              pts_coord_camj = Rw2j.dot(pts_coord_world) + Tw2j\n",
        "              pixel_coord_camj = np.matmul(K, pts_coord_camj)\n",
        "\n",
        "              valid_idxj = np.where(np.logical_and.reduce((pixel_coord_camj[2]>0,\n",
        "                                                          pixel_coord_camj[0]/pixel_coord_camj[2]>=0,\n",
        "                                                          pixel_coord_camj[0]/pixel_coord_camj[2]<=W-1,\n",
        "                                                          pixel_coord_camj[1]/pixel_coord_camj[2]>=0,\n",
        "                                                          pixel_coord_camj[1]/pixel_coord_camj[2]<=H-1)))[0]\n",
        "              pts_depthsj = pixel_coord_camj[-1:, valid_idxj]\n",
        "              pixel_coord_camj = pixel_coord_camj[:2, valid_idxj]/pixel_coord_camj[-1:, valid_idxj]\n",
        "              round_coord_camj = np.round(pixel_coord_camj).astype(np.int32)\n",
        "\n",
        "\n",
        "              x, y = np.meshgrid(np.arange(W, dtype=np.float32), np.arange(H, dtype=np.float32), indexing='xy')\n",
        "              grid = np.stack((x,y), axis=-1).reshape(-1,2)\n",
        "              imagej = interp_grid(pixel_coord_camj.transpose(1,0), pts_colors[valid_idxj], grid, method='linear', fill_value=0).reshape(H,W,3)\n",
        "              imagej = edgemask[...,None]*imagej + (1-edgemask[...,None])*np.pad(imagej[1:-1,1:-1], ((1,1),(1,1),(0,0)), mode='edge')\n",
        "\n",
        "              depthj = interp_grid(pixel_coord_camj.transpose(1,0), pts_depthsj.T, grid, method='linear', fill_value=0).reshape(H,W)\n",
        "              depthj = edgemask*depthj + (1-edgemask)*np.pad(depthj[1:-1,1:-1], ((1,1),(1,1)), mode='edge')\n",
        "\n",
        "              maskj = np.zeros((H,W), dtype=np.float32)\n",
        "              maskj[round_coord_camj[1], round_coord_camj[0]] = 1\n",
        "              maskj = maximum_filter(maskj, size=(9,9), axes=(0,1))\n",
        "              imagej = maskj[...,None]*imagej + (1-maskj[...,None])*(-1)\n",
        "\n",
        "              maskj = minimum_filter((imagej.sum(-1)!=-3)*1, size=(11,11), axes=(0,1))\n",
        "              imagej = maskj[...,None]*imagej + (1-maskj[...,None])*0\n",
        "\n",
        "              traindata['frames'].append({\n",
        "                  'image': Image.fromarray(np.round(imagej*255.).astype(np.uint8)),\n",
        "                  'transform_matrix': Pc2w.tolist(),\n",
        "              })\n",
        "\n",
        "      progress(1, desc='Baking Gaussians...')\n",
        "      return traindata"
      ],
      "metadata": {
        "id": "O7a1PjNLeN9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "ld = LucidDreamer()\n",
        "ips = [\n",
        "  'bedroom',\n",
        "  '/content/LucidDreamer-Gaussian/examples/04.png',\n",
        "  'a bedroom',\n",
        "  'photo frame, frame, boarder, simple color, inconsistent',\n",
        "  'lookaround',\n",
        "  4,\n",
        "  25,\n",
        "  'llff',\n",
        "  'SD1.5 (default)']\n",
        "\n",
        "print(\"开始生成3D场景...\")\n",
        "result_ply_file, result_default_gallery = ld.run(Image.open(ips[1]), ips[2], ips[3], ips[4], ips[5], ips[6], ips[7])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "70c781d19d144267a20bea33072349a3",
            "df49d824c52d4b56ab9b32d42c3933e3",
            "71b5be11ea564cd6800e506555195d86",
            "fd77231aec5a444da0f8d2094d796178",
            "4b7496653a48444f90802d0787efabe6",
            "482f3f06e04a4f76ac332762357ff43e",
            "0f0c2359781b49709d56c8ee314527e1",
            "8549ca1839104e5f9252ad624f545982",
            "0bacdc0a693d4cf388931195b7cbeb79",
            "8a6e5dc4e5c2431e962d03a95f9165ef",
            "6057175799bd414c839080b1a1d2341b"
          ]
        },
        "id": "PJE-2nZSezvA",
        "outputId": "e43d3185-7315-4c66-d274-d4e01751058d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70c781d19d144267a20bea33072349a3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "id": "VljYcYwfwDRD",
        "outputId": "700ec79b-ed07-4dac-83c7-a8d78bb1dee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-24 07:02:38.543577: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-24 07:02:38.788836: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-24 07:02:38.858974: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-24 07:02:41.182202: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]An error occurred while trying to fetch /content/LucidDreamer-Gaussian/stablediffusion/SD1-5/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /content/LucidDreamer-Gaussian/stablediffusion/SD1-5/unet.\n",
            "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
            "Loading pipeline components...:  43% 3/7 [00:22<00:23,  5.95s/it]An error occurred while trying to fetch /content/LucidDreamer-Gaussian/stablediffusion/SD1-5/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /content/LucidDreamer-Gaussian/stablediffusion/SD1-5/vae.\n",
            "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
            "Loading pipeline components...: 100% 7/7 [00:32<00:00,  4.59s/it]\n",
            "img_size [384, 512]\n",
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n",
            "Params passed to Resize transform:\n",
            "\twidth:  512\n",
            "\theight:  384\n",
            "\tresize_target:  True\n",
            "\tkeep_aspect_ratio:  True\n",
            "\tensure_multiple_of:  32\n",
            "\tresize_method:  minimal\n",
            "Using pretrained resource url::https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_N.pt\n",
            "Loaded successfully\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://ea8ac69ea944a292c2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "100% 30/30 [00:04<00:00,  6.00it/s]\n",
            "100% 30/30 [00:04<00:00,  6.50it/s]\n",
            "100% 30/30 [00:04<00:00,  6.47it/s]\n",
            "100% 30/30 [00:04<00:00,  6.48it/s]\n",
            "100% 30/30 [00:04<00:00,  6.46it/s]\n",
            "100% 30/30 [00:04<00:00,  6.41it/s]\n",
            "100% 30/30 [00:04<00:00,  6.37it/s]\n",
            "100% 30/30 [00:04<00:00,  6.39it/s]\n",
            "100% 30/30 [00:04<00:00,  6.37it/s]\n",
            "100% 30/30 [00:04<00:00,  6.35it/s]\n",
            "100% 30/30 [00:04<00:00,  6.28it/s]\n",
            "100% 30/30 [00:04<00:00,  6.25it/s]\n",
            "100% 30/30 [00:04<00:00,  6.25it/s]\n",
            "100% 30/30 [00:04<00:00,  6.23it/s]\n",
            "100% 30/30 [00:04<00:00,  6.18it/s]\n",
            "100% 30/30 [00:04<00:00,  6.19it/s]\n",
            "100% 30/30 [00:04<00:00,  6.23it/s]\n",
            "100% 30/30 [00:04<00:00,  6.20it/s]\n",
            "100% 30/30 [00:04<00:00,  6.15it/s]\n",
            "1 / 105\n",
            "2 / 105\n",
            "3 / 105\n",
            "4 / 105\n",
            "5 / 105\n",
            "6 / 105\n",
            "7 / 105\n",
            "8 / 105\n",
            "9 / 105\n",
            "10 / 105\n",
            "11 / 105\n",
            "12 / 105\n",
            "13 / 105\n",
            "14 / 105\n",
            "15 / 105\n",
            "16 / 105\n",
            "17 / 105\n",
            "18 / 105\n",
            "19 / 105\n",
            "20 / 105\n",
            "21 / 105\n",
            "22 / 105\n",
            "23 / 105\n",
            "24 / 105\n",
            "25 / 105\n",
            "26 / 105\n",
            "27 / 105\n",
            "28 / 105\n",
            "29 / 105\n",
            "30 / 105\n",
            "31 / 105\n",
            "32 / 105\n",
            "33 / 105\n",
            "34 / 105\n",
            "35 / 105\n",
            "36 / 105\n",
            "37 / 105\n",
            "38 / 105\n",
            "39 / 105\n",
            "40 / 105\n",
            "41 / 105\n",
            "42 / 105\n",
            "43 / 105\n",
            "44 / 105\n",
            "45 / 105\n",
            "46 / 105\n",
            "47 / 105\n",
            "48 / 105\n",
            "49 / 105\n",
            "50 / 105\n",
            "51 / 105\n",
            "52 / 105\n",
            "53 / 105\n",
            "54 / 105\n",
            "55 / 105\n",
            "56 / 105\n",
            "57 / 105\n",
            "58 / 105\n",
            "59 / 105\n",
            "60 / 105\n",
            "61 / 105\n",
            "62 / 105\n",
            "63 / 105\n",
            "64 / 105\n",
            "65 / 105\n",
            "66 / 105\n",
            "67 / 105\n",
            "68 / 105\n",
            "69 / 105\n",
            "70 / 105\n",
            "71 / 105\n",
            "72 / 105\n",
            "73 / 105\n",
            "74 / 105\n",
            "75 / 105\n",
            "76 / 105\n",
            "77 / 105\n",
            "78 / 105\n",
            "79 / 105\n",
            "80 / 105\n",
            "81 / 105\n",
            "82 / 105\n",
            "83 / 105\n",
            "84 / 105\n",
            "85 / 105\n",
            "86 / 105\n",
            "87 / 105\n",
            "88 / 105\n",
            "89 / 105\n",
            "90 / 105\n",
            "91 / 105\n",
            "92 / 105\n",
            "93 / 105\n",
            "94 / 105\n",
            "95 / 105\n",
            "96 / 105\n",
            "97 / 105\n",
            "98 / 105\n",
            "99 / 105\n",
            "100 / 105\n",
            "Reading Training Transforms\n",
            "Loading Training Cameras\n",
            "Loading Preset Cameras\n",
            "Number of points at initialisation :  1587532\n",
            "100% 2990/2990 [03:52<00:00, 12.85it/s]\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qc7jAh4pocVa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "70c781d19d144267a20bea33072349a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df49d824c52d4b56ab9b32d42c3933e3",
              "IPY_MODEL_71b5be11ea564cd6800e506555195d86",
              "IPY_MODEL_fd77231aec5a444da0f8d2094d796178"
            ],
            "layout": "IPY_MODEL_4b7496653a48444f90802d0787efabe6",
            "tabbable": null,
            "tooltip": null
          }
        },
        "df49d824c52d4b56ab9b32d42c3933e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_482f3f06e04a4f76ac332762357ff43e",
            "placeholder": "​",
            "style": "IPY_MODEL_0f0c2359781b49709d56c8ee314527e1",
            "tabbable": null,
            "tooltip": null,
            "value": "Loading pipeline components...:   0%"
          }
        },
        "71b5be11ea564cd6800e506555195d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_8549ca1839104e5f9252ad624f545982",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bacdc0a693d4cf388931195b7cbeb79",
            "tabbable": null,
            "tooltip": null,
            "value": 0
          }
        },
        "fd77231aec5a444da0f8d2094d796178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_8a6e5dc4e5c2431e962d03a95f9165ef",
            "placeholder": "​",
            "style": "IPY_MODEL_6057175799bd414c839080b1a1d2341b",
            "tabbable": null,
            "tooltip": null,
            "value": " 0/7 [00:00&lt;?, ?it/s]"
          }
        },
        "4b7496653a48444f90802d0787efabe6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "482f3f06e04a4f76ac332762357ff43e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f0c2359781b49709d56c8ee314527e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "8549ca1839104e5f9252ad624f545982": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bacdc0a693d4cf388931195b7cbeb79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a6e5dc4e5c2431e962d03a95f9165ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6057175799bd414c839080b1a1d2341b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}