{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuanSnowing/colab/blob/main/LucidDreamer_Gaussian_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VjYy0F2gZIPR",
        "outputId": "9c2a28d6-c13d-42b6-d42f-99ba259e0744",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LucidDreamer-Gaussian'...\n",
            "remote: Enumerating objects: 442, done.\u001b[K\n",
            "remote: Counting objects: 100% (145/145), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 442 (delta 124), reused 103 (delta 103), pack-reused 297 (from 1)\u001b[K\n",
            "Receiving objects: 100% (442/442), 62.50 MiB | 21.90 MiB/s, done.\n",
            "Resolving deltas: 100% (187/187), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone --recursive -b dev https://github.com/camenduru/LucidDreamer-Gaussian\n",
        "# !pip install timm==0.6.7\n",
        "# !pip install peft diffusers scipy numpy imageio[ffmpeg] opencv-python Pillow open3d torch==2.0.1 torchvision==0.15.2 gradio omegaconf\n",
        "\n",
        "# !pip install plyfile==0.8.1\n",
        "# !pip install -q diffusers accelerate gradio open3d plyfile timm==0.6.12\n",
        "# !pip install -q https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl\n",
        "\n",
        "# !apt install libglm-dev\n",
        "# !pip install /content/LucidDreamer-Gaussian/submodules/depth-diff-gaussian-rasterization-min\n",
        "# !pip install /content/LucidDreamer-Gaussian/submodules/simple-knn\n",
        "\n",
        "# !apt -y install -qq aria2\n",
        "# !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/ZoeDepth/resolve/main/ZoeD_M12_N.pt -d /root/.cache/torch/hub/checkpoints -o ZoeD_M12_N.pt\n",
        "\n",
        "# !sed -i 's/\\(new_height, new_width\\)/int(new_height), int(new_width)/g' /content/LucidDreamer-Gaussian/ZoeDepth/zoedepth/models/base_models/midas.py\n",
        "\n",
        "# import torch\n",
        "\n",
        "# torch.hub.load('LucidDreamer-Gaussian/ZoeDepth', 'ZoeD_N', source='local', pretrained=True, strict=False).to('cuda')\n",
        "# !sed -i 's/\\(new_height, new_width\\)/int(new_height), int(new_width)/g' /root/.cache/torch/hub/intel-isl_MiDaS_master/midas/backbones/beit.py\n",
        "\n",
        "# %cd /content/LucidDreamer-Gaussian\n",
        "# import os\n",
        "# import glob\n",
        "# import platform\n",
        "# import pathlib\n",
        "# import shlex\n",
        "# import subprocess\n",
        "# import gradio as gr\n",
        "# from huggingface_hub import snapshot_download\n",
        "\n",
        "\n",
        "# root = '/content/LucidDreamer-Gaussian'\n",
        "# example_root = os.path.join(root, 'examples')\n",
        "# ckpt_root = os.path.join(root, 'stablediffusion')\n",
        "# use_symlinks = False\n",
        "\n",
        "# d = example_root\n",
        "# if len(glob.glob(os.path.join(d, '*.ply'))) < 8:\n",
        "#     snapshot_download(repo_id=\"ironjr/LucidDreamerDemo\", repo_type=\"model\", local_dir=d, local_dir_use_symlinks=use_symlinks)\n",
        "# d = os.path.join(ckpt_root, 'Blazing Drive V11m')\n",
        "# if not os.path.exists(d):\n",
        "#     snapshot_download(repo_id=\"ironjr/BlazingDriveV11m\", repo_type=\"model\", local_dir=d, local_dir_use_symlinks=use_symlinks)\n",
        "# d = os.path.join(ckpt_root, 'RealCartoon-Pixar V5')\n",
        "# if not os.path.exists(d):\n",
        "#     snapshot_download(repo_id=\"ironjr/RealCartoon-PixarV5\", repo_type=\"model\", local_dir=d, local_dir_use_symlinks=use_symlinks)\n",
        "# d = os.path.join(ckpt_root, 'Realistic Vision V5.1')\n",
        "# if not os.path.exists(d):\n",
        "#     snapshot_download(repo_id=\"ironjr/RealisticVisionV5-1\", repo_type=\"model\", local_dir=d, local_dir_use_symlinks=use_symlinks)\n",
        "# d = os.path.join(ckpt_root, 'SD1-5')\n",
        "# if not os.path.exists(d):\n",
        "#     snapshot_download(repo_id=\"runwayml/stable-diffusion-inpainting\", repo_type=\"model\", local_dir=d, local_dir_use_symlinks=use_symlinks)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "from random import randint\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "import imageio\n",
        "import numpy as np\n",
        "import open3d as o3d\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from scipy.interpolate import griddata as interp_grid\n",
        "from scipy.ndimage import minimum_filter, maximum_filter\n",
        "\n",
        "import torch\n",
        "import gradio as gr\n",
        "from diffusers import StableDiffusionInpaintPipeline, StableDiffusionPipeline\n",
        "\n",
        "from arguments import GSParams, CameraParams\n",
        "from gaussian_renderer import render\n",
        "from scene import Scene, GaussianModel\n",
        "from scene.dataset_readers import loadCameraPreset\n",
        "from utils.loss import l1_loss, ssim\n",
        "from utils.camera import load_json\n",
        "from utils.trajectory import get_camerapaths, get_pcdGenPoses\n",
        "\n",
        "\n",
        "class LucidDreamer:\n",
        "  def __init__(self, savepath='./'):\n",
        "      self.opt = GSParams()\n",
        "      self.cam = CameraParams()\n",
        "      self.savepath = savepath\n",
        "      if not os.path.exists(self.savepath):\n",
        "          os.makedirs(self.savepath, exist_ok=True)\n",
        "\n",
        "      self.gaussians = GaussianModel(self.opt.sh_degree)\n",
        "\n",
        "      bg_color = [1, 1, 1] if self.opt.white_background else [0, 0, 0]\n",
        "      self.background = torch.tensor(bg_color, dtype=torch.float32, device='cuda')\n",
        "\n",
        "      self.rgb = StableDiffusionInpaintPipeline.from_pretrained(\n",
        "          'runwayml/stable-diffusion-inpainting', revision='fp16', torch_dtype=torch.float16).to('cuda')\n",
        "      self.rgb.enable_xformers_memory_efficient_attention()\n",
        "      self.d = torch.hub.load('./ZoeDepth', 'ZoeD_N', source='local', pretrained=True).to('cuda')\n",
        "\n",
        "  def run(self, rgb_cond, txt_cond, neg_txt_cond, pcdgenpath, seed, diff_steps, render_camerapath):\n",
        "      gaussians, default_gallery = self.create(rgb_cond, txt_cond, neg_txt_cond, pcdgenpath, seed, diff_steps)\n",
        "      gallery = self.render_video(render_camerapath)\n",
        "      return (gaussians, default_gallery, gallery)\n",
        "\n",
        "  def create(self, rgb_cond, txt_cond, neg_txt_cond, pcdgenpath, seed, diff_steps):\n",
        "      self.traindata = self.generate_pcd(rgb_cond, txt_cond, neg_txt_cond, pcdgenpath, seed, diff_steps)\n",
        "      self.scene = Scene(self.traindata, self.gaussians, self.opt)\n",
        "      self.training()\n",
        "      outfile = self.save_ply()\n",
        "      default_gallery = self.render_video('LLFF')\n",
        "      return outfile, default_gallery\n",
        "\n",
        "  def save_ply(self):\n",
        "      self.scene.gaussians.save_ply(os.path.join(self.savepath, 'gaussiansplatting.ply'))\n",
        "      return os.path.join(self.savepath, 'gaussiansplatting.ply')\n",
        "\n",
        "  def render_video(self, preset, gaussians=None, traindata_path=None):\n",
        "      videofname = os.path.join(self.savepath, f'video_{preset}_60fps.mp4')\n",
        "\n",
        "      if not hasattr(self, 'gaussians'):\n",
        "          self.gaussians = gaussians\n",
        "\n",
        "      if not hasattr(self, 'scene'):\n",
        "          print(traindata_path)\n",
        "\n",
        "          with open(traindata_path, 'r') as f:\n",
        "              traindata = json.load(traindata_path)\n",
        "\n",
        "          print(traindata)\n",
        "\n",
        "          view_total = loadCameraPreset(traindata=traindata, presetdata=get_camerapaths())\n",
        "          views = view_total[preset]\n",
        "          print(view_total.keys())\n",
        "      else:\n",
        "          views = self.scene.getPresetCameras(preset)\n",
        "\n",
        "      framelist = []\n",
        "      depthlist = []\n",
        "      dmin, dmax = 1e3, 0\n",
        "      for view in views:\n",
        "          results = render(view, self.gaussians, self.opt, self.background)\n",
        "          rendering = results['render']\n",
        "\n",
        "          dmask = (results['depth']>0)\n",
        "          depth = (results['depth']*dmask).detach().cpu().numpy()\n",
        "          depthlist.append(depth)\n",
        "          if results['depth'][dmask].min().item() < dmin:\n",
        "              dmin = results['depth'][dmask].min().item()\n",
        "          if results['depth'][dmask].max().item() > dmax:\n",
        "              dmax = results['depth'][dmask].max().item()\n",
        "\n",
        "          framelist.append(np.round(rendering.permute(1,2,0).detach().cpu().numpy().clip(0,1)*255.).astype(np.uint8))\n",
        "\n",
        "      imageio.mimwrite(videofname, framelist, fps=60, quality=8)\n",
        "      return videofname\n",
        "\n",
        "  def training(self):\n",
        "      if not self.scene:\n",
        "          raise('Build 3D Scene First!')\n",
        "\n",
        "      for iteration in tqdm(range(1, self.opt.iterations + 1)):\n",
        "          self.gaussians.update_learning_rate(iteration)\n",
        "\n",
        "          # Every 1000 its we increase the levels of SH up to a maximum degree\n",
        "          if iteration % 1000 == 0:\n",
        "              self.gaussians.oneupSHdegree()\n",
        "\n",
        "          # Pick a random Camera\n",
        "          viewpoint_stack = self.scene.getTrainCameras().copy()\n",
        "          viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack)-1))\n",
        "\n",
        "          # Render\n",
        "          render_pkg = render(viewpoint_cam, self.gaussians, self.opt, self.background)\n",
        "          image, viewspace_point_tensor, visibility_filter, radii = (\n",
        "              render_pkg['render'], render_pkg['viewspace_points'], render_pkg['visibility_filter'], render_pkg['radii'])\n",
        "\n",
        "          # Loss\n",
        "          gt_image = viewpoint_cam.original_image.cuda()\n",
        "          Ll1 = l1_loss(image, gt_image)\n",
        "          loss = (1.0 - self.opt.lambda_dssim) * Ll1 + self.opt.lambda_dssim * (1.0 - ssim(image, gt_image))\n",
        "          loss.backward()\n",
        "\n",
        "          with torch.no_grad():\n",
        "              # Densification\n",
        "              if iteration < self.opt.densify_until_iter:\n",
        "                  # Keep track of max radii in image-space for pruning\n",
        "                  self.gaussians.max_radii2D[visibility_filter] = torch.max(\n",
        "                      self.gaussians.max_radii2D[visibility_filter], radii[visibility_filter])\n",
        "                  self.gaussians.add_densification_stats(viewspace_point_tensor, visibility_filter)\n",
        "\n",
        "                  if iteration > self.opt.densify_from_iter and iteration % self.opt.densification_interval == 0:\n",
        "                      size_threshold = 20 if iteration > self.opt.opacity_reset_interval else None\n",
        "                      self.gaussians.densify_and_prune(\n",
        "                          self.opt.densify_grad_threshold, 0.005, self.scene.cameras_extent, size_threshold)\n",
        "\n",
        "                  if (iteration % self.opt.opacity_reset_interval == 0\n",
        "                      or (self.opt.white_background and iteration == self.opt.densify_from_iter)\n",
        "                  ):\n",
        "                      self.gaussians.reset_opacity()\n",
        "\n",
        "              # Optimizer step\n",
        "              if iteration < self.opt.iterations:\n",
        "                  self.gaussians.optimizer.step()\n",
        "                  self.gaussians.optimizer.zero_grad(set_to_none = True)\n",
        "\n",
        "  def generate_pcd(self, rgb_cond, prompt, negative_prompt, pcdgenpath, seed, diff_steps, progress=gr.Progress()):\n",
        "      ## processing inputs\n",
        "      generator=torch.Generator(device='cuda').manual_seed(seed)\n",
        "\n",
        "      w_in, h_in = rgb_cond.size\n",
        "      if w_in/h_in > 1.1 or h_in/w_in > 1.1: # if height and width are similar, do center crop\n",
        "          in_res = max(w_in, h_in)\n",
        "          image_in, mask_in = np.zeros((in_res, in_res, 3), dtype=np.uint8), 255*np.ones((in_res, in_res, 3), dtype=np.uint8)\n",
        "          image_in[int(in_res/2-h_in/2):int(in_res/2+h_in/2), int(in_res/2-w_in/2):int(in_res/2+w_in/2)] = np.array(rgb_cond)\n",
        "          mask_in[int(in_res/2-h_in/2):int(in_res/2+h_in/2), int(in_res/2-w_in/2):int(in_res/2+w_in/2)] = 0\n",
        "          image_curr = self.rgb(prompt=prompt, negative_prompt=negative_prompt, generator=generator,\n",
        "                            image=Image.fromarray(image_in).resize((self.cam.W, self.cam.H)),\n",
        "                            mask_image=Image.fromarray(mask_in).resize((self.cam.W, self.cam.H))).images[0]\n",
        "\n",
        "      else: # if there is a large gap between height and width, do inpainting\n",
        "          if w_in > h_in:\n",
        "              image_curr = rgb_cond.crop((int(w_in/2-h_in/2), 0, int(w_in/2+h_in/2), h_in)).resize((self.cam.W, self.cam.H))\n",
        "          else: # w <= h\n",
        "              image_curr = rgb_cond.crop((0, int(h_in/2-w_in/2), w_in, int(h_in/2+w_in/2))).resize((self.cam.W, self.cam.H))\n",
        "\n",
        "      render_poses = get_pcdGenPoses(pcdgenpath)\n",
        "      depth_curr = self.d.infer_pil(image_curr)\n",
        "      center_depth = np.mean(depth_curr[h_in//2-10:h_in//2+10, w_in//2-10:w_in//2+10])\n",
        "\n",
        "      ###########################################################################################################################\n",
        "      # Iterative scene generation\n",
        "      H, W, K = self.cam.H, self.cam.W, self.cam.K\n",
        "\n",
        "      x, y = np.meshgrid(np.arange(W, dtype=np.float32), np.arange(H, dtype=np.float32), indexing='xy') # pixels\n",
        "      edgeN = 2\n",
        "      edgemask = np.ones((H-2*edgeN, W-2*edgeN))\n",
        "      edgemask = np.pad(edgemask, ((edgeN,edgeN),(edgeN,edgeN)))\n",
        "\n",
        "      ### initialize\n",
        "      R0, T0 = render_poses[0,:3,:3], render_poses[0,:3,3:4]\n",
        "      pts_coord_cam = np.matmul(np.linalg.inv(K), np.stack((x*depth_curr, y*depth_curr, 1*depth_curr), axis=0).reshape(3,-1))\n",
        "      new_pts_coord_world2 = (np.linalg.inv(R0).dot(pts_coord_cam) - np.linalg.inv(R0).dot(T0)).astype(np.float32)\n",
        "      new_pts_colors2 = (np.array(image_curr).reshape(-1,3).astype(np.float32)/255.)\n",
        "\n",
        "      pts_coord_world, pts_colors = new_pts_coord_world2.copy(), new_pts_colors2.copy()\n",
        "\n",
        "      progress(0, desc='Dreaming...')\n",
        "      time.sleep(0.5)\n",
        "\n",
        "      for j, tqdm_i in enumerate(progress.tqdm(range(1, len(render_poses)), desc='Dreaming')):\n",
        "          i = j + 1\n",
        "          if i == len(render_poses) - 1:\n",
        "              break\n",
        "          R, T = render_poses[i,:3,:3], render_poses[i,:3,3:4]\n",
        "\n",
        "          ### Transform world to pixel\n",
        "          pts_coord_cam2 = R.dot(pts_coord_world) + T\n",
        "          pixel_coord_cam2 = np.matmul(K, pts_coord_cam2)\n",
        "\n",
        "          valid_idx = np.where(np.logical_and.reduce((pixel_coord_cam2[2]>0,\n",
        "                                                      pixel_coord_cam2[0]/pixel_coord_cam2[2]>=0,\n",
        "                                                      pixel_coord_cam2[0]/pixel_coord_cam2[2]<=W-1,\n",
        "                                                      pixel_coord_cam2[1]/pixel_coord_cam2[2]>=0,\n",
        "                                                      pixel_coord_cam2[1]/pixel_coord_cam2[2]<=H-1)))[0]\n",
        "          pixel_coord_cam2 = pixel_coord_cam2[:2, valid_idx]/pixel_coord_cam2[-1:, valid_idx]\n",
        "          round_coord_cam2 = np.round(pixel_coord_cam2).astype(np.int32)\n",
        "\n",
        "          x, y = np.meshgrid(np.arange(W, dtype=np.float32), np.arange(H, dtype=np.float32), indexing='xy')\n",
        "          grid = np.stack((x,y), axis=-1).reshape(-1,2)\n",
        "          image2 = interp_grid(pixel_coord_cam2.transpose(1,0), pts_colors[valid_idx], grid, method='linear', fill_value=0).reshape(H,W,3)\n",
        "          image2 = edgemask[...,None]*image2 + (1-edgemask[...,None])*np.pad(image2[1:-1,1:-1], ((1,1),(1,1),(0,0)), mode='edge')\n",
        "\n",
        "          round_mask2 = np.zeros((H,W), dtype=np.float32)\n",
        "          round_mask2[round_coord_cam2[1], round_coord_cam2[0]] = 1\n",
        "\n",
        "          round_mask2 = maximum_filter(round_mask2, size=(9,9), axes=(0,1))\n",
        "          image2 = round_mask2[...,None]*image2 + (1-round_mask2[...,None])*(-1)\n",
        "\n",
        "          mask2 = minimum_filter((image2.sum(-1)!=-3)*1, size=(11,11), axes=(0,1))\n",
        "          image2 = mask2[...,None]*image2 + (1-mask2[...,None])*0\n",
        "\n",
        "          mask_hf = np.abs(mask2[:H-1, :W-1] - mask2[1:, :W-1]) + np.abs(mask2[:H-1, :W-1] - mask2[:H-1, 1:])\n",
        "          mask_hf = np.pad(mask_hf, ((0,1), (0,1)), 'edge')\n",
        "          mask_hf = np.where(mask_hf < 0.3, 0, 1)\n",
        "          border_valid_idx = np.where(mask_hf[round_coord_cam2[1], round_coord_cam2[0]] == 1)[0]\n",
        "\n",
        "          image_curr = self.rgb(prompt=prompt, negative_prompt=negative_prompt, generator=generator, num_inference_steps= diff_steps,\n",
        "                                  image=Image.fromarray(np.round(image2*255.).astype(np.uint8)),\n",
        "                                  mask_image=Image.fromarray(np.round((1-mask2[:,:])*255.).astype(np.uint8))).images[0]\n",
        "          depth_curr = self.d.infer_pil(image_curr)\n",
        "\n",
        "          t_z2 = torch.tensor(depth_curr)\n",
        "          sc = torch.ones(1).float().requires_grad_(True)\n",
        "          optimizer = torch.optim.Adam(params=[sc], lr=0.001)\n",
        "\n",
        "          for idx in range(100):\n",
        "              trans3d = torch.tensor([[sc,0,0,0], [0,sc,0,0], [0,0,sc,0], [0,0,0,1]]).requires_grad_(True)\n",
        "              coord_cam2 = torch.matmul(torch.tensor(np.linalg.inv(K)), torch.stack((torch.tensor(x)*t_z2, torch.tensor(y)*t_z2, 1*t_z2), axis=0)[:,round_coord_cam2[1], round_coord_cam2[0]].reshape(3,-1))\n",
        "              coord_world2 = (torch.tensor(np.linalg.inv(R)).float().matmul(coord_cam2) - torch.tensor(np.linalg.inv(R)).float().matmul(torch.tensor(T).float()))\n",
        "              coord_world2_warp = torch.cat((coord_world2, torch.ones((1,valid_idx.shape[0]))), dim=0)\n",
        "              coord_world2_trans = torch.matmul(trans3d, coord_world2_warp)\n",
        "              coord_world2_trans = coord_world2_trans[:3] / coord_world2_trans[-1]\n",
        "              loss = torch.mean((torch.tensor(pts_coord_world[:,valid_idx]).float() - coord_world2_trans)**2)\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "          with torch.no_grad():\n",
        "              coord_cam2 = torch.matmul(torch.tensor(np.linalg.inv(K)), torch.stack((torch.tensor(x)*t_z2, torch.tensor(y)*t_z2, 1*t_z2), axis=0)[:,round_coord_cam2[1, border_valid_idx], round_coord_cam2[0, border_valid_idx]].reshape(3,-1))\n",
        "              coord_world2 = (torch.tensor(np.linalg.inv(R)).float().matmul(coord_cam2) - torch.tensor(np.linalg.inv(R)).float().matmul(torch.tensor(T).float()))\n",
        "              coord_world2_warp = torch.cat((coord_world2, torch.ones((1, border_valid_idx.shape[0]))), dim=0)\n",
        "              coord_world2_trans = torch.matmul(trans3d, coord_world2_warp)\n",
        "              coord_world2_trans = coord_world2_trans[:3] / coord_world2_trans[-1]\n",
        "\n",
        "          trans3d = trans3d.detach().numpy()\n",
        "\n",
        "          pts_coord_cam2 = np.matmul(np.linalg.inv(K), np.stack((x*depth_curr, y*depth_curr, 1*depth_curr), axis=0).reshape(3,-1))[:,np.where(1-mask2.reshape(-1))[0]]\n",
        "          camera_origin_coord_world2 = - np.linalg.inv(R).dot(T).astype(np.float32)\n",
        "          new_pts_coord_world2 = (np.linalg.inv(R).dot(pts_coord_cam2) - np.linalg.inv(R).dot(T)).astype(np.float32)\n",
        "          new_pts_coord_world2_warp = np.concatenate((new_pts_coord_world2, np.ones((1, new_pts_coord_world2.shape[1]))), axis=0)\n",
        "          new_pts_coord_world2 = np.matmul(trans3d, new_pts_coord_world2_warp)\n",
        "          new_pts_coord_world2 = new_pts_coord_world2[:3] / new_pts_coord_world2[-1]\n",
        "          new_pts_colors2 = (np.array(image_curr).reshape(-1,3).astype(np.float32)/255.)[np.where(1-mask2.reshape(-1))[0]]\n",
        "\n",
        "          vector_camorigin_to_campixels = coord_world2_trans.detach().numpy() - camera_origin_coord_world2\n",
        "          vector_camorigin_to_pcdpixels = pts_coord_world[:,valid_idx[border_valid_idx]] - camera_origin_coord_world2\n",
        "\n",
        "          compensate_depth_coeff = np.sum(vector_camorigin_to_pcdpixels * vector_camorigin_to_campixels, axis=0) / np.sum(vector_camorigin_to_campixels * vector_camorigin_to_campixels, axis=0) # N_correspond\n",
        "          compensate_pts_coord_world2_correspond = camera_origin_coord_world2 + vector_camorigin_to_campixels * compensate_depth_coeff.reshape(1,-1)\n",
        "\n",
        "          compensate_coord_cam2_correspond = R.dot(compensate_pts_coord_world2_correspond) + T\n",
        "          homography_coord_cam2_correspond = R.dot(coord_world2_trans.detach().numpy()) + T\n",
        "\n",
        "          compensate_depth_correspond = compensate_coord_cam2_correspond[-1] - homography_coord_cam2_correspond[-1]\n",
        "          compensate_depth_zero = np.zeros(4)\n",
        "          compensate_depth = np.concatenate((compensate_depth_correspond, compensate_depth_zero), axis=0)\n",
        "\n",
        "          pixel_cam2_correspond = pixel_coord_cam2[:, border_valid_idx]\n",
        "          pixel_cam2_zero = np.array([[0,0,W-1,W-1],[0,H-1,0,H-1]])\n",
        "          pixel_cam2 = np.concatenate((pixel_cam2_correspond, pixel_cam2_zero), axis=1).transpose(1,0)\n",
        "\n",
        "          # Calculate for masked pixels\n",
        "          masked_pixels_xy = np.stack(np.where(1-mask2), axis=1)[:, [1,0]]\n",
        "          new_depth_linear, new_depth_nearest = interp_grid(pixel_cam2, compensate_depth, masked_pixels_xy), interp_grid(pixel_cam2, compensate_depth, masked_pixels_xy, method='nearest')\n",
        "          new_depth = np.where(np.isnan(new_depth_linear), new_depth_nearest, new_depth_linear)\n",
        "\n",
        "          pts_coord_cam2 = np.matmul(np.linalg.inv(K), np.stack((x*depth_curr, y*depth_curr, 1*depth_curr), axis=0).reshape(3,-1))[:,np.where(1-mask2.reshape(-1))[0]]\n",
        "          x_nonmask, y_nonmask = x.reshape(-1)[np.where(1-mask2.reshape(-1))[0]], y.reshape(-1)[np.where(1-mask2.reshape(-1))[0]]\n",
        "          compensate_pts_coord_cam2 = np.matmul(np.linalg.inv(K), np.stack((x_nonmask*new_depth, y_nonmask*new_depth, 1*new_depth), axis=0))\n",
        "          new_warp_pts_coord_cam2 = pts_coord_cam2 + compensate_pts_coord_cam2\n",
        "\n",
        "          new_pts_coord_world2 = (np.linalg.inv(R).dot(new_warp_pts_coord_cam2) - np.linalg.inv(R).dot(T)).astype(np.float32)\n",
        "          new_pts_coord_world2_warp = np.concatenate((new_pts_coord_world2, np.ones((1, new_pts_coord_world2.shape[1]))), axis=0)\n",
        "          new_pts_coord_world2 = np.matmul(trans3d, new_pts_coord_world2_warp)\n",
        "          new_pts_coord_world2 = new_pts_coord_world2[:3] / new_pts_coord_world2[-1]\n",
        "          new_pts_colors2 = (np.array(image_curr).reshape(-1,3).astype(np.float32)/255.)[np.where(1-mask2.reshape(-1))[0]]\n",
        "\n",
        "          pts_coord_world = np.concatenate((pts_coord_world, new_pts_coord_world2), axis=-1)\n",
        "          pts_colors = np.concatenate((pts_colors, new_pts_colors2), axis=0)\n",
        "\n",
        "      #################################################################################################\n",
        "\n",
        "      yz_reverse = np.array([[1,0,0], [0,-1,0], [0,0,-1]])\n",
        "      traindata = {\n",
        "          'camera_angle_x': self.cam.fov[0],\n",
        "          'W': W,\n",
        "          'H': H,\n",
        "          'pcd_points': pts_coord_world,\n",
        "          'pcd_colors': pts_colors,\n",
        "          'frames': [],\n",
        "      }\n",
        "\n",
        "      internel_render_poses = get_pcdGenPoses('hemisphere', {'center_depth': center_depth})\n",
        "\n",
        "      progress(0, desc='Aligning...')\n",
        "      time.sleep(0.5)\n",
        "\n",
        "      for i, tqdm_i in enumerate(progress.tqdm(range(len(render_poses)), desc='Aligning')):\n",
        "          if i == len(render_poses) - 1:\n",
        "              break\n",
        "          for j in range(len(internel_render_poses)):\n",
        "              idx = i * len(internel_render_poses) + j\n",
        "              print(f'{idx+1} / {len(render_poses)*len(internel_render_poses)}')\n",
        "\n",
        "              ### Transform world to pixel\n",
        "              Rw2i = render_poses[i,:3,:3]\n",
        "              Tw2i = render_poses[i,:3,3:4]\n",
        "              Ri2j = internel_render_poses[j,:3,:3]\n",
        "              Ti2j = internel_render_poses[j,:3,3:4]\n",
        "\n",
        "              Rw2j = np.matmul(Ri2j, Rw2i)\n",
        "              Tw2j = np.matmul(Ri2j, Tw2i) + Ti2j\n",
        "\n",
        "              # Transfrom cam2 to world + change sign of yz axis\n",
        "              Rj2w = np.matmul(yz_reverse, Rw2j).T\n",
        "              Tj2w = -np.matmul(Rj2w, np.matmul(yz_reverse, Tw2j))\n",
        "              Pc2w = np.concatenate((Rj2w, Tj2w), axis=1)\n",
        "              Pc2w = np.concatenate((Pc2w, np.array([[0,0,0,1]])), axis=0)\n",
        "\n",
        "              pts_coord_camj = Rw2j.dot(pts_coord_world) + Tw2j\n",
        "              pixel_coord_camj = np.matmul(K, pts_coord_camj)\n",
        "\n",
        "              valid_idxj = np.where(np.logical_and.reduce((pixel_coord_camj[2]>0,\n",
        "                                                          pixel_coord_camj[0]/pixel_coord_camj[2]>=0,\n",
        "                                                          pixel_coord_camj[0]/pixel_coord_camj[2]<=W-1,\n",
        "                                                          pixel_coord_camj[1]/pixel_coord_camj[2]>=0,\n",
        "                                                          pixel_coord_camj[1]/pixel_coord_camj[2]<=H-1)))[0]\n",
        "              pts_depthsj = pixel_coord_camj[-1:, valid_idxj]\n",
        "              pixel_coord_camj = pixel_coord_camj[:2, valid_idxj]/pixel_coord_camj[-1:, valid_idxj]\n",
        "              round_coord_camj = np.round(pixel_coord_camj).astype(np.int32)\n",
        "\n",
        "\n",
        "              x, y = np.meshgrid(np.arange(W, dtype=np.float32), np.arange(H, dtype=np.float32), indexing='xy')\n",
        "              grid = np.stack((x,y), axis=-1).reshape(-1,2)\n",
        "              imagej = interp_grid(pixel_coord_camj.transpose(1,0), pts_colors[valid_idxj], grid, method='linear', fill_value=0).reshape(H,W,3)\n",
        "              imagej = edgemask[...,None]*imagej + (1-edgemask[...,None])*np.pad(imagej[1:-1,1:-1], ((1,1),(1,1),(0,0)), mode='edge')\n",
        "\n",
        "              depthj = interp_grid(pixel_coord_camj.transpose(1,0), pts_depthsj.T, grid, method='linear', fill_value=0).reshape(H,W)\n",
        "              depthj = edgemask*depthj + (1-edgemask)*np.pad(depthj[1:-1,1:-1], ((1,1),(1,1)), mode='edge')\n",
        "\n",
        "              maskj = np.zeros((H,W), dtype=np.float32)\n",
        "              maskj[round_coord_camj[1], round_coord_camj[0]] = 1\n",
        "              maskj = maximum_filter(maskj, size=(9,9), axes=(0,1))\n",
        "              imagej = maskj[...,None]*imagej + (1-maskj[...,None])*(-1)\n",
        "\n",
        "              maskj = minimum_filter((imagej.sum(-1)!=-3)*1, size=(11,11), axes=(0,1))\n",
        "              imagej = maskj[...,None]*imagej + (1-maskj[...,None])*0\n",
        "\n",
        "              traindata['frames'].append({\n",
        "                  'image': Image.fromarray(np.round(imagej*255.).astype(np.uint8)),\n",
        "                  'transform_matrix': Pc2w.tolist(),\n",
        "              })\n",
        "\n",
        "      progress(1, desc='Baking Gaussians...')\n",
        "      return traindata"
      ],
      "metadata": {
        "id": "O7a1PjNLeN9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from luciddreamer import LucidDreamer\n",
        "ld = LucidDreamer()\n",
        "\n",
        "image_path = \"\"\n",
        "image = Image.open(image_path)\n",
        "ips = [\n",
        "  'bedroom',\n",
        "  'examples/04.png',\n",
        "  'a bedroom',\n",
        "  'photo frame, frame, boarder, simple color, inconsistent',\n",
        "  'lookaround',\n",
        "  4,\n",
        "  25,\n",
        "  'llff',\n",
        "  'SD1.5 (default)',]\n",
        "fn=ld.create, inputs=ips[1:-2] + ips[-1:] + ips[:1], outputs=[result_ply_file]\n",
        "run_button.click(fn=ld.run, inputs=ips[1:] + ips[:1], outputs=[result_ply_file, result_gallery, result_depth])\n",
        "print(\"开始生成3D场景...\")\n",
        "result_ply_file, result_default_gallery, result_gallery = ld.run(\n",
        "    image, prompt, n_prompt, camerapath, seed, steps, render_camerapath\n",
        ")\n",
        "\n",
        "    # Step 3: 保存生成的文件到服务器目录\n",
        "    output_directory = \"output_results\"\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    ply_output_path = os.path.join(output_directory, \"result.ply\")\n",
        "    with open(ply_output_path, 'wb') as f:\n",
        "        f.write(result_ply_file)  # 假设 result_ply_file 是二进制文件\n",
        "\n",
        "    print(f\"3D PLY 文件已保存到: {ply_output_path}\")\n",
        "    print(\"您可以下载该文件。\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_scene()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "70c781d19d144267a20bea33072349a3",
            "df49d824c52d4b56ab9b32d42c3933e3",
            "71b5be11ea564cd6800e506555195d86",
            "fd77231aec5a444da0f8d2094d796178",
            "4b7496653a48444f90802d0787efabe6",
            "482f3f06e04a4f76ac332762357ff43e",
            "0f0c2359781b49709d56c8ee314527e1",
            "8549ca1839104e5f9252ad624f545982",
            "0bacdc0a693d4cf388931195b7cbeb79",
            "8a6e5dc4e5c2431e962d03a95f9165ef",
            "6057175799bd414c839080b1a1d2341b"
          ]
        },
        "id": "PJE-2nZSezvA",
        "outputId": "e43d3185-7315-4c66-d274-d4e01751058d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70c781d19d144267a20bea33072349a3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "id": "VljYcYwfwDRD",
        "outputId": "700ec79b-ed07-4dac-83c7-a8d78bb1dee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-24 07:02:38.543577: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-24 07:02:38.788836: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-24 07:02:38.858974: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-24 07:02:41.182202: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]An error occurred while trying to fetch /content/LucidDreamer-Gaussian/stablediffusion/SD1-5/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /content/LucidDreamer-Gaussian/stablediffusion/SD1-5/unet.\n",
            "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
            "Loading pipeline components...:  43% 3/7 [00:22<00:23,  5.95s/it]An error occurred while trying to fetch /content/LucidDreamer-Gaussian/stablediffusion/SD1-5/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /content/LucidDreamer-Gaussian/stablediffusion/SD1-5/vae.\n",
            "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
            "Loading pipeline components...: 100% 7/7 [00:32<00:00,  4.59s/it]\n",
            "img_size [384, 512]\n",
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n",
            "Params passed to Resize transform:\n",
            "\twidth:  512\n",
            "\theight:  384\n",
            "\tresize_target:  True\n",
            "\tkeep_aspect_ratio:  True\n",
            "\tensure_multiple_of:  32\n",
            "\tresize_method:  minimal\n",
            "Using pretrained resource url::https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_N.pt\n",
            "Loaded successfully\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://ea8ac69ea944a292c2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "100% 30/30 [00:04<00:00,  6.00it/s]\n",
            "100% 30/30 [00:04<00:00,  6.50it/s]\n",
            "100% 30/30 [00:04<00:00,  6.47it/s]\n",
            "100% 30/30 [00:04<00:00,  6.48it/s]\n",
            "100% 30/30 [00:04<00:00,  6.46it/s]\n",
            "100% 30/30 [00:04<00:00,  6.41it/s]\n",
            "100% 30/30 [00:04<00:00,  6.37it/s]\n",
            "100% 30/30 [00:04<00:00,  6.39it/s]\n",
            "100% 30/30 [00:04<00:00,  6.37it/s]\n",
            "100% 30/30 [00:04<00:00,  6.35it/s]\n",
            "100% 30/30 [00:04<00:00,  6.28it/s]\n",
            "100% 30/30 [00:04<00:00,  6.25it/s]\n",
            "100% 30/30 [00:04<00:00,  6.25it/s]\n",
            "100% 30/30 [00:04<00:00,  6.23it/s]\n",
            "100% 30/30 [00:04<00:00,  6.18it/s]\n",
            "100% 30/30 [00:04<00:00,  6.19it/s]\n",
            "100% 30/30 [00:04<00:00,  6.23it/s]\n",
            "100% 30/30 [00:04<00:00,  6.20it/s]\n",
            "100% 30/30 [00:04<00:00,  6.15it/s]\n",
            "1 / 105\n",
            "2 / 105\n",
            "3 / 105\n",
            "4 / 105\n",
            "5 / 105\n",
            "6 / 105\n",
            "7 / 105\n",
            "8 / 105\n",
            "9 / 105\n",
            "10 / 105\n",
            "11 / 105\n",
            "12 / 105\n",
            "13 / 105\n",
            "14 / 105\n",
            "15 / 105\n",
            "16 / 105\n",
            "17 / 105\n",
            "18 / 105\n",
            "19 / 105\n",
            "20 / 105\n",
            "21 / 105\n",
            "22 / 105\n",
            "23 / 105\n",
            "24 / 105\n",
            "25 / 105\n",
            "26 / 105\n",
            "27 / 105\n",
            "28 / 105\n",
            "29 / 105\n",
            "30 / 105\n",
            "31 / 105\n",
            "32 / 105\n",
            "33 / 105\n",
            "34 / 105\n",
            "35 / 105\n",
            "36 / 105\n",
            "37 / 105\n",
            "38 / 105\n",
            "39 / 105\n",
            "40 / 105\n",
            "41 / 105\n",
            "42 / 105\n",
            "43 / 105\n",
            "44 / 105\n",
            "45 / 105\n",
            "46 / 105\n",
            "47 / 105\n",
            "48 / 105\n",
            "49 / 105\n",
            "50 / 105\n",
            "51 / 105\n",
            "52 / 105\n",
            "53 / 105\n",
            "54 / 105\n",
            "55 / 105\n",
            "56 / 105\n",
            "57 / 105\n",
            "58 / 105\n",
            "59 / 105\n",
            "60 / 105\n",
            "61 / 105\n",
            "62 / 105\n",
            "63 / 105\n",
            "64 / 105\n",
            "65 / 105\n",
            "66 / 105\n",
            "67 / 105\n",
            "68 / 105\n",
            "69 / 105\n",
            "70 / 105\n",
            "71 / 105\n",
            "72 / 105\n",
            "73 / 105\n",
            "74 / 105\n",
            "75 / 105\n",
            "76 / 105\n",
            "77 / 105\n",
            "78 / 105\n",
            "79 / 105\n",
            "80 / 105\n",
            "81 / 105\n",
            "82 / 105\n",
            "83 / 105\n",
            "84 / 105\n",
            "85 / 105\n",
            "86 / 105\n",
            "87 / 105\n",
            "88 / 105\n",
            "89 / 105\n",
            "90 / 105\n",
            "91 / 105\n",
            "92 / 105\n",
            "93 / 105\n",
            "94 / 105\n",
            "95 / 105\n",
            "96 / 105\n",
            "97 / 105\n",
            "98 / 105\n",
            "99 / 105\n",
            "100 / 105\n",
            "Reading Training Transforms\n",
            "Loading Training Cameras\n",
            "Loading Preset Cameras\n",
            "Number of points at initialisation :  1587532\n",
            "100% 2990/2990 [03:52<00:00, 12.85it/s]\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qc7jAh4pocVa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "70c781d19d144267a20bea33072349a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df49d824c52d4b56ab9b32d42c3933e3",
              "IPY_MODEL_71b5be11ea564cd6800e506555195d86",
              "IPY_MODEL_fd77231aec5a444da0f8d2094d796178"
            ],
            "layout": "IPY_MODEL_4b7496653a48444f90802d0787efabe6",
            "tabbable": null,
            "tooltip": null
          }
        },
        "df49d824c52d4b56ab9b32d42c3933e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_482f3f06e04a4f76ac332762357ff43e",
            "placeholder": "​",
            "style": "IPY_MODEL_0f0c2359781b49709d56c8ee314527e1",
            "tabbable": null,
            "tooltip": null,
            "value": "Loading pipeline components...:   0%"
          }
        },
        "71b5be11ea564cd6800e506555195d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_8549ca1839104e5f9252ad624f545982",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bacdc0a693d4cf388931195b7cbeb79",
            "tabbable": null,
            "tooltip": null,
            "value": 0
          }
        },
        "fd77231aec5a444da0f8d2094d796178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_8a6e5dc4e5c2431e962d03a95f9165ef",
            "placeholder": "​",
            "style": "IPY_MODEL_6057175799bd414c839080b1a1d2341b",
            "tabbable": null,
            "tooltip": null,
            "value": " 0/7 [00:00&lt;?, ?it/s]"
          }
        },
        "4b7496653a48444f90802d0787efabe6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "482f3f06e04a4f76ac332762357ff43e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f0c2359781b49709d56c8ee314527e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "8549ca1839104e5f9252ad624f545982": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bacdc0a693d4cf388931195b7cbeb79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a6e5dc4e5c2431e962d03a95f9165ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6057175799bd414c839080b1a1d2341b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}